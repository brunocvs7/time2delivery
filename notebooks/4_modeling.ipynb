{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.models.predict_model' from '/Users/bruno.santos/Desktop/Estudos/case_cornershop/time2delivery/utils/models/predict_model.py'>"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(utils.models.predict_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libs\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import optuna\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression, QuantileRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error\n",
    "from utils.features.build import build_distance, build_hour_group\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from ngboost import NGBRegressor\n",
    "from sklearn.model_selection import cross_validate, cross_val_score\n",
    "from ngboost.distns import Exponential, Normal, LogNormal\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from utils.models.evaluate import plot_learning_curve, plot_permutation_importance\n",
    "from utils.models.predict_model import get_intervals\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Env variables and data\n",
    "load_dotenv(find_dotenv())\n",
    "DATA_INPUT_PATH = os.getenv('DATA_PROCESSED_PATH')\n",
    "DATA_TRAIN_NAME = 'train_best_features'\n",
    "DATA_TEST_NAME = 'test'\n",
    "DATA_SUBMISSION = 'submission'\n",
    "# Data\n",
    "df_orders_train = pd.read_parquet(os.path.join(DATA_INPUT_PATH, DATA_TRAIN_NAME))\n",
    "df_orders_test = pd.read_parquet(os.path.join(DATA_INPUT_PATH, DATA_TEST_NAME))\n",
    "df_submission = pd.read_parquet(os.path.join(DATA_INPUT_PATH, DATA_SUBMISSION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step We'll try 3 approaches to solve the problem:\n",
    "- Linear Regression \n",
    "- Random Forest \n",
    "- NGBoost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Linear Regression We intend to create a baseline to be beaten. We'll observe performance in terms of Mean Absolute Percentage Error and Proportion of overestimated predictions. \n",
    "The former metric will be better if It approaches 0, the latter will be better if When model gets It wrong, the model overestimates instead of underestimate. We wish this property because When the model underestimate, the order will be late, and It is better to be early than late. As We saw in Exploratory Data Analysis, the `total_minutes` distribution has a long tail, so It may be import to apply logarithm in order to relieve the impact of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating X and y for training\n",
    "X_train = df_orders_train.drop('total_minutes', axis=1)\n",
    "y_train = df_orders_train['total_minutes']\n",
    "# list with all columns\n",
    "all_columns = X_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will have 3 imputers: Median for some numerical, Mode for categorical and 0 for some numerical\n",
    "median_impute_columns_indexes = [all_columns.index(x) for x in ['n_distinct_items', 'distance_km', 'found_rate']]\n",
    "mode_impute_columns_indexes = [all_columns.index(x) for x in ['on_demand', 'hour_group']]\n",
    "zero_impute_columns_indexes = [all_columns.index(x) for x in ['sum_kgs', 'sum_unities']]\n",
    "cat_columns = df_orders_train.select_dtypes(include=['O']).columns.tolist()\n",
    "cat_columns_indexes = [all_columns.index(x) for x in cat_columns]\n",
    "num_columns = df_orders_train.drop('total_minutes', axis=1).select_dtypes(include=['int32', 'int64', 'float32', 'float64']).columns.tolist()\n",
    "num_columns_indexes = [all_columns.index(x) for x in num_columns]\n",
    "impute_median_columns = ['n_distinct_items', 'distance_km', 'found_rate']\n",
    "impute_zero_columns = ['sum_kgs', 'sum_unities']\n",
    "# Feature engineering steps\n",
    "distance_transformer = FunctionTransformer(func=build_distance)\n",
    "hour_group_transformer = FunctionTransformer(func=build_hour_group)\n",
    "pipe_feature_engineering = Pipeline(steps=[('distance_transformer', distance_transformer),\n",
    "                                           ('hor_group_transformer', hour_group_transformer)])\n",
    "# Imputation Steps\n",
    "\n",
    "impute_median = Pipeline([('impute_median', SimpleImputer(strategy='median'))])\n",
    "impute_mode = Pipeline([('impute_mode', SimpleImputer(strategy='most_frequent'))])\n",
    "impute_zero = Pipeline([('impute_zero', SimpleImputer(strategy='constant', fill_value=0))])\n",
    "one_hot = Pipeline([('cat_encoder', OneHotEncoder(handle_unknown='ignore'))])\n",
    "cat_transformer= Pipeline([('impute_mode', impute_mode),                                \n",
    "                           ('one_hot', one_hot)])\n",
    "preprocessor = ColumnTransformer([('impute_mode_one_hot', cat_transformer, cat_columns),\n",
    "                                      ('impute_median', impute_median, impute_median_columns),\n",
    "                                      ('impute_zero', impute_zero, impute_zero_columns)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([('impute_mode', SimpleImputer(strategy='most_frequent'),cat_columns)\n",
    "                                      ('one_hot', OneHotEncoder(handle_unknown='ignore'), cat_columns),\n",
    "                                      ('impute_median', SimpleImputer(strategy='median'), impute_median_columns),\n",
    "                                      ('impute_zero', SimpleImputer(strategy='constant', fill_value=0), impute_zero_columns)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our baseline will be a Linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline with preprocessor and Model\n",
    "model_baseline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                  ('model', TransformedTargetRegressor(LinearRegression(), func=np.log, inverse_func=np.exp))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing a 10-Fold cross validation to get an estimate for MAE and MAPE\n",
    "df_baseline_cv_metrics = pd.DataFrame(cross_validate(model_baseline,\n",
    "                                        X_train,\n",
    "                                        y_train, \n",
    "                                        scoring=['neg_mean_absolute_error', 'neg_mean_absolute_percentage_error'], \n",
    "                                        return_train_score=True, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_neg_mean_absolute_error</th>\n",
       "      <th>train_neg_mean_absolute_error</th>\n",
       "      <th>test_neg_mean_absolute_percentage_error</th>\n",
       "      <th>train_neg_mean_absolute_percentage_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.037881</td>\n",
       "      <td>0.009463</td>\n",
       "      <td>-20.022554</td>\n",
       "      <td>-20.415693</td>\n",
       "      <td>-0.265349</td>\n",
       "      <td>-0.267731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.025508</td>\n",
       "      <td>0.007558</td>\n",
       "      <td>-19.512231</td>\n",
       "      <td>-20.478940</td>\n",
       "      <td>-0.262205</td>\n",
       "      <td>-0.268664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.018187</td>\n",
       "      <td>0.006447</td>\n",
       "      <td>-20.663146</td>\n",
       "      <td>-20.334302</td>\n",
       "      <td>-0.270654</td>\n",
       "      <td>-0.267280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.017918</td>\n",
       "      <td>0.006165</td>\n",
       "      <td>-19.108091</td>\n",
       "      <td>-20.530233</td>\n",
       "      <td>-0.247190</td>\n",
       "      <td>-0.269626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.019595</td>\n",
       "      <td>0.006614</td>\n",
       "      <td>-20.154597</td>\n",
       "      <td>-20.398224</td>\n",
       "      <td>-0.280051</td>\n",
       "      <td>-0.267034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.016729</td>\n",
       "      <td>0.006390</td>\n",
       "      <td>-21.316906</td>\n",
       "      <td>-20.262441</td>\n",
       "      <td>-0.266537</td>\n",
       "      <td>-0.267155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.017145</td>\n",
       "      <td>0.005698</td>\n",
       "      <td>-20.798057</td>\n",
       "      <td>-20.336376</td>\n",
       "      <td>-0.271078</td>\n",
       "      <td>-0.267501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.016684</td>\n",
       "      <td>0.005619</td>\n",
       "      <td>-21.100928</td>\n",
       "      <td>-20.275357</td>\n",
       "      <td>-0.276072</td>\n",
       "      <td>-0.266705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.016558</td>\n",
       "      <td>0.005443</td>\n",
       "      <td>-20.361290</td>\n",
       "      <td>-20.383473</td>\n",
       "      <td>-0.269171</td>\n",
       "      <td>-0.267875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.019485</td>\n",
       "      <td>0.006053</td>\n",
       "      <td>-21.155491</td>\n",
       "      <td>-20.292296</td>\n",
       "      <td>-0.274145</td>\n",
       "      <td>-0.266546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_neg_mean_absolute_error  \\\n",
       "0  0.037881    0.009463                    -20.022554   \n",
       "1  0.025508    0.007558                    -19.512231   \n",
       "2  0.018187    0.006447                    -20.663146   \n",
       "3  0.017918    0.006165                    -19.108091   \n",
       "4  0.019595    0.006614                    -20.154597   \n",
       "5  0.016729    0.006390                    -21.316906   \n",
       "6  0.017145    0.005698                    -20.798057   \n",
       "7  0.016684    0.005619                    -21.100928   \n",
       "8  0.016558    0.005443                    -20.361290   \n",
       "9  0.019485    0.006053                    -21.155491   \n",
       "\n",
       "   train_neg_mean_absolute_error  test_neg_mean_absolute_percentage_error  \\\n",
       "0                     -20.415693                                -0.265349   \n",
       "1                     -20.478940                                -0.262205   \n",
       "2                     -20.334302                                -0.270654   \n",
       "3                     -20.530233                                -0.247190   \n",
       "4                     -20.398224                                -0.280051   \n",
       "5                     -20.262441                                -0.266537   \n",
       "6                     -20.336376                                -0.271078   \n",
       "7                     -20.275357                                -0.276072   \n",
       "8                     -20.383473                                -0.269171   \n",
       "9                     -20.292296                                -0.274145   \n",
       "\n",
       "   train_neg_mean_absolute_percentage_error  \n",
       "0                                 -0.267731  \n",
       "1                                 -0.268664  \n",
       "2                                 -0.267280  \n",
       "3                                 -0.269626  \n",
       "4                                 -0.267034  \n",
       "5                                 -0.267155  \n",
       "6                                 -0.267501  \n",
       "7                                 -0.266705  \n",
       "8                                 -0.267875  \n",
       "9                                 -0.266546  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_baseline_cv_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('impute_mode_one_hot',\n",
       "                                                  Pipeline(steps=[('impute_mode',\n",
       "                                                                   Pipeline(steps=[('impute_mode',\n",
       "                                                                                    SimpleImputer(strategy='most_frequent'))])),\n",
       "                                                                  ('one_hot',\n",
       "                                                                   Pipeline(steps=[('cat_encoder',\n",
       "                                                                                    OneHotEncoder(handle_unknown='ignore'))]))]),\n",
       "                                                  ['on_demand', 'hour_group']),\n",
       "                                                 ('impute_median',\n",
       "                                                  Pipeline(steps=[('impute_median',\n",
       "                                                                   SimpleImputer(strategy='median'))]),\n",
       "                                                  ['n_distinct_items',\n",
       "                                                   'distance_km',\n",
       "                                                   'found_rate']),\n",
       "                                                 ('impute_zero',\n",
       "                                                  Pipeline(steps=[('impute_zero',\n",
       "                                                                   SimpleImputer(fill_value=0,\n",
       "                                                                                 strategy='constant'))]),\n",
       "                                                  ['sum_kgs',\n",
       "                                                   'sum_unities'])])),\n",
       "                ('model',\n",
       "                 TransformedTargetRegressor(func=<ufunc 'log'>,\n",
       "                                            inverse_func=<ufunc 'exp'>,\n",
       "                                            regressor=LinearRegression()))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_baseline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see average MAPE and Standard Deviation of MAPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MAPE - Cross Validation Test: -0.26824515384366066\n",
      "Standard Deviation of MAPE - Cross Validation Test: 0.009075169386691588\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average MAPE - Cross Validation Test: {df_baseline_cv_metrics['test_neg_mean_absolute_percentage_error'].mean()}\")\n",
    "print(f\"Standard Deviation of MAPE - Cross Validation Test: {df_baseline_cv_metrics['test_neg_mean_absolute_percentage_error'].std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, nn average, the model is wrong by 26.8%. It means that for an order that will take 100 minutes to finish, It tends to predict either 127 or 73 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, We need to see if the predictions are higher or lower than the real value. It's better to overestimate the time, because the order will not be late. Let's check what happens more in our case, overestimate or underestimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About 51.33333333333333% of our predictions are higher than the real value\n"
     ]
    }
   ],
   "source": [
    "# Generating predictions for X_train\n",
    "y_train_baseline_predict = model_baseline.predict(X_train)\n",
    "# Calculating proportion of overestimation\n",
    "print(f'About {((y_train_baseline_predict >= y_train).sum())/len(y_train)*100}% of our predictions are higher than the real value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a challenger to the baseline, We will train a Random Forest Regressor with Bayesian Optimization to tuning the hyperparameters. This model will have higher variance compared to Linear Regression and We'll focus on improving MAPE and also try to improve the rate of overestimated orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-08 07:58:06,797]\u001b[0m A new study created in memory with name: no-name-4a823511-951f-4656-a16a-aecd3824805a\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 07:58:32,982]\u001b[0m Trial 0 finished with value: -0.10407002929617515 and parameters: {'max_depth': 7, 'max_leaf_nodes': 823, 'n_estimators': 782}. Best is trial 0 with value: -0.10407002929617515.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 07:58:44,517]\u001b[0m Trial 1 finished with value: -0.10427455992137806 and parameters: {'max_depth': 8, 'max_leaf_nodes': 874, 'n_estimators': 303}. Best is trial 0 with value: -0.10407002929617515.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 07:59:05,826]\u001b[0m Trial 2 finished with value: -0.10528881882322012 and parameters: {'max_depth': 5, 'max_leaf_nodes': 787, 'n_estimators': 831}. Best is trial 0 with value: -0.10407002929617515.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 07:59:29,360]\u001b[0m Trial 3 finished with value: -0.10425633132417547 and parameters: {'max_depth': 8, 'max_leaf_nodes': 660, 'n_estimators': 643}. Best is trial 0 with value: -0.10407002929617515.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 07:59:45,740]\u001b[0m Trial 4 finished with value: -0.10472258452255848 and parameters: {'max_depth': 9, 'max_leaf_nodes': 620, 'n_estimators': 395}. Best is trial 0 with value: -0.10407002929617515.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 07:59:52,593]\u001b[0m Trial 5 finished with value: -0.10403844253892167 and parameters: {'max_depth': 7, 'max_leaf_nodes': 889, 'n_estimators': 219}. Best is trial 5 with value: -0.10403844253892167.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:00:19,022]\u001b[0m Trial 6 finished with value: -0.10408241880968852 and parameters: {'max_depth': 7, 'max_leaf_nodes': 764, 'n_estimators': 834}. Best is trial 5 with value: -0.10403844253892167.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:00:29,174]\u001b[0m Trial 7 finished with value: -0.10543043931814489 and parameters: {'max_depth': 10, 'max_leaf_nodes': 988, 'n_estimators': 236}. Best is trial 5 with value: -0.10403844253892167.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:00:42,765]\u001b[0m Trial 8 finished with value: -0.10766074380690589 and parameters: {'max_depth': 4, 'max_leaf_nodes': 692, 'n_estimators': 590}. Best is trial 5 with value: -0.10403844253892167.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:00:54,533]\u001b[0m Trial 9 finished with value: -0.1339374730627758 and parameters: {'max_depth': 1, 'max_leaf_nodes': 667, 'n_estimators': 764}. Best is trial 5 with value: -0.10403844253892167.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:00:59,585]\u001b[0m Trial 10 finished with value: -0.11234361567496347 and parameters: {'max_depth': 3, 'max_leaf_nodes': 889, 'n_estimators': 272}. Best is trial 5 with value: -0.10403844253892167.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:01:14,112]\u001b[0m Trial 11 finished with value: -0.10425370873310083 and parameters: {'max_depth': 6, 'max_leaf_nodes': 971, 'n_estimators': 519}. Best is trial 5 with value: -0.10403844253892167.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:01:44,037]\u001b[0m Trial 12 finished with value: -0.10474274494263658 and parameters: {'max_depth': 9, 'max_leaf_nodes': 904, 'n_estimators': 743}. Best is trial 5 with value: -0.10403844253892167.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:01:47,129]\u001b[0m Trial 13 finished with value: -0.10404656818980691 and parameters: {'max_depth': 7, 'max_leaf_nodes': 791, 'n_estimators': 89}. Best is trial 5 with value: -0.10403844253892167.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:01:50,738]\u001b[0m Trial 14 finished with value: -0.10493763128786675 and parameters: {'max_depth': 9, 'max_leaf_nodes': 742, 'n_estimators': 82}. Best is trial 5 with value: -0.10403844253892167.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:01:53,238]\u001b[0m Trial 15 finished with value: -0.10773489572965647 and parameters: {'max_depth': 4, 'max_leaf_nodes': 706, 'n_estimators': 111}. Best is trial 5 with value: -0.10403844253892167.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:01:56,038]\u001b[0m Trial 16 finished with value: -0.10426323248954532 and parameters: {'max_depth': 6, 'max_leaf_nodes': 930, 'n_estimators': 89}. Best is trial 5 with value: -0.10403844253892167.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:01:58,282]\u001b[0m Trial 17 finished with value: -0.10433661696874982 and parameters: {'max_depth': 6, 'max_leaf_nodes': 810, 'n_estimators': 73}. Best is trial 5 with value: -0.10403844253892167.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:02:06,809]\u001b[0m Trial 18 finished with value: -0.13389222825896713 and parameters: {'max_depth': 1, 'max_leaf_nodes': 998, 'n_estimators': 894}. Best is trial 5 with value: -0.10403844253892167.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:02:20,021]\u001b[0m Trial 19 finished with value: -0.10425135717824892 and parameters: {'max_depth': 6, 'max_leaf_nodes': 786, 'n_estimators': 444}. Best is trial 5 with value: -0.10403844253892167.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:02:40,405]\u001b[0m Trial 20 finished with value: -0.10475860957126817 and parameters: {'max_depth': 9, 'max_leaf_nodes': 788, 'n_estimators': 472}. Best is trial 5 with value: -0.10403844253892167.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:03:16,467]\u001b[0m Trial 21 finished with value: -0.10425106413894066 and parameters: {'max_depth': 8, 'max_leaf_nodes': 906, 'n_estimators': 957}. Best is trial 5 with value: -0.10403844253892167.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:03:38,975]\u001b[0m Trial 22 finished with value: -0.10528839132454967 and parameters: {'max_depth': 5, 'max_leaf_nodes': 909, 'n_estimators': 833}. Best is trial 5 with value: -0.10403844253892167.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:04:18,638]\u001b[0m Trial 23 finished with value: -0.10528039521606729 and parameters: {'max_depth': 10, 'max_leaf_nodes': 726, 'n_estimators': 959}. Best is trial 5 with value: -0.10403844253892167.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:04:23,137]\u001b[0m Trial 24 finished with value: -0.10434077572078079 and parameters: {'max_depth': 8, 'max_leaf_nodes': 853, 'n_estimators': 130}. Best is trial 5 with value: -0.10403844253892167.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:04:28,361]\u001b[0m Trial 25 finished with value: -0.13401070808348267 and parameters: {'max_depth': 1, 'max_leaf_nodes': 802, 'n_estimators': 579}. Best is trial 5 with value: -0.10403844253892167.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:04:37,786]\u001b[0m Trial 26 finished with value: -0.10401060572931267 and parameters: {'max_depth': 7, 'max_leaf_nodes': 718, 'n_estimators': 309}. Best is trial 26 with value: -0.10401060572931267.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:04:46,094]\u001b[0m Trial 27 finished with value: -0.1042420968278116 and parameters: {'max_depth': 6, 'max_leaf_nodes': 712, 'n_estimators': 300}. Best is trial 26 with value: -0.10401060572931267.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:04:52,296]\u001b[0m Trial 28 finished with value: -0.1042463046112253 and parameters: {'max_depth': 6, 'max_leaf_nodes': 842, 'n_estimators': 218}. Best is trial 26 with value: -0.10401060572931267.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:05:02,241]\u001b[0m Trial 29 finished with value: -0.10427699869850846 and parameters: {'max_depth': 8, 'max_leaf_nodes': 735, 'n_estimators': 291}. Best is trial 26 with value: -0.10401060572931267.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:05:05,005]\u001b[0m Trial 30 finished with value: -0.10406880998679222 and parameters: {'max_depth': 7, 'max_leaf_nodes': 661, 'n_estimators': 88}. Best is trial 26 with value: -0.10401060572931267.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:05:08,505]\u001b[0m Trial 31 finished with value: -0.10425702598886748 and parameters: {'max_depth': 6, 'max_leaf_nodes': 683, 'n_estimators': 127}. Best is trial 26 with value: -0.10401060572931267.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:05:12,036]\u001b[0m Trial 32 finished with value: -0.10406176465186645 and parameters: {'max_depth': 7, 'max_leaf_nodes': 620, 'n_estimators': 111}. Best is trial 26 with value: -0.10401060572931267.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:05:17,882]\u001b[0m Trial 33 finished with value: -0.1054664583075404 and parameters: {'max_depth': 10, 'max_leaf_nodes': 642, 'n_estimators': 128}. Best is trial 26 with value: -0.10401060572931267.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:05:27,614]\u001b[0m Trial 34 finished with value: -0.10423387522675068 and parameters: {'max_depth': 6, 'max_leaf_nodes': 615, 'n_estimators': 328}. Best is trial 26 with value: -0.10401060572931267.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:05:32,204]\u001b[0m Trial 35 finished with value: -0.10404240504982312 and parameters: {'max_depth': 7, 'max_leaf_nodes': 601, 'n_estimators': 137}. Best is trial 26 with value: -0.10401060572931267.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:05:42,317]\u001b[0m Trial 36 finished with value: -0.10401584533953072 and parameters: {'max_depth': 7, 'max_leaf_nodes': 897, 'n_estimators': 315}. Best is trial 26 with value: -0.10401060572931267.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:05:54,118]\u001b[0m Trial 37 finished with value: -0.10402818918443635 and parameters: {'max_depth': 7, 'max_leaf_nodes': 889, 'n_estimators': 371}. Best is trial 26 with value: -0.10401060572931267.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:06:06,629]\u001b[0m Trial 38 finished with value: -0.10424376144394602 and parameters: {'max_depth': 8, 'max_leaf_nodes': 910, 'n_estimators': 336}. Best is trial 26 with value: -0.10401060572931267.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:06:18,128]\u001b[0m Trial 39 finished with value: -0.10402042474039441 and parameters: {'max_depth': 7, 'max_leaf_nodes': 928, 'n_estimators': 349}. Best is trial 26 with value: -0.10401060572931267.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:06:32,728]\u001b[0m Trial 40 finished with value: -0.10425455532510775 and parameters: {'max_depth': 6, 'max_leaf_nodes': 870, 'n_estimators': 546}. Best is trial 26 with value: -0.10401060572931267.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:06:39,816]\u001b[0m Trial 41 finished with value: -0.10403975076767044 and parameters: {'max_depth': 7, 'max_leaf_nodes': 989, 'n_estimators': 233}. Best is trial 26 with value: -0.10401060572931267.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:06:50,922]\u001b[0m Trial 42 finished with value: -0.10402101074103016 and parameters: {'max_depth': 7, 'max_leaf_nodes': 893, 'n_estimators': 343}. Best is trial 26 with value: -0.10401060572931267.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:07:02,169]\u001b[0m Trial 43 finished with value: -0.10525496839268925 and parameters: {'max_depth': 5, 'max_leaf_nodes': 900, 'n_estimators': 446}. Best is trial 26 with value: -0.10401060572931267.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:07:15,739]\u001b[0m Trial 44 finished with value: -0.10403572610324152 and parameters: {'max_depth': 7, 'max_leaf_nodes': 888, 'n_estimators': 426}. Best is trial 26 with value: -0.10401060572931267.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:07:31,143]\u001b[0m Trial 45 finished with value: -0.10423865156911463 and parameters: {'max_depth': 8, 'max_leaf_nodes': 939, 'n_estimators': 422}. Best is trial 26 with value: -0.10401060572931267.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:07:44,885]\u001b[0m Trial 46 finished with value: -0.1042520650927046 and parameters: {'max_depth': 8, 'max_leaf_nodes': 855, 'n_estimators': 382}. Best is trial 26 with value: -0.10401060572931267.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:07:53,627]\u001b[0m Trial 47 finished with value: -0.10401393414790708 and parameters: {'max_depth': 7, 'max_leaf_nodes': 889, 'n_estimators': 275}. Best is trial 26 with value: -0.10401060572931267.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:08:16,249]\u001b[0m Trial 48 finished with value: -0.10429219119879711 and parameters: {'max_depth': 6, 'max_leaf_nodes': 600, 'n_estimators': 826}. Best is trial 26 with value: -0.10401060572931267.\u001b[0m\n",
      "\u001b[32m[I 2021-12-08 08:08:23,214]\u001b[0m Trial 49 finished with value: -0.104026539591568 and parameters: {'max_depth': 7, 'max_leaf_nodes': 915, 'n_estimators': 224}. Best is trial 26 with value: -0.10401060572931267.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning using optuna \n",
    "def objective(trial):\n",
    "    params = {'model__regressor__max_depth':trial.suggest_int('max_depth', 1, 10),\n",
    "              'model__regressor__max_leaf_nodes':trial.suggest_int('max_leaf_nodes', 600, 1000),\n",
    "              'model__regressor__n_estimators':trial.suggest_int('n_estimators', 30, 1000)}\n",
    "    \n",
    "    model_rf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                  ('model', TransformedTargetRegressor(RandomForestRegressor(random_state=123), func=np.log, inverse_func=np.exp))])\n",
    "    model_rf.set_params(**params)   \n",
    "    score = cross_val_score(model_rf, X_train, np.log(y_train), cv=5, scoring='neg_mean_squared_error')\n",
    "    mse = score.mean()\n",
    "\n",
    "    return mse\n",
    "sampler = TPESampler(multivariate=True)\n",
    "study = optuna.create_study(sampler=sampler, direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                  ('model', RandomForestRegressor(random_state=123))])\n",
    "model_rf['model'].set_params(**study.best_params)\n",
    "df_rf_cv_metrics = pd.DataFrame(cross_validate(model_rf,\n",
    "                                        X_train,\n",
    "                                        y_train, \n",
    "                                        scoring=['neg_mean_absolute_error', 'neg_mean_absolute_percentage_error'], \n",
    "                                        return_train_score=True, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_neg_mean_absolute_error</th>\n",
       "      <th>train_neg_mean_absolute_error</th>\n",
       "      <th>test_neg_mean_absolute_percentage_error</th>\n",
       "      <th>train_neg_mean_absolute_percentage_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.186526</td>\n",
       "      <td>0.036434</td>\n",
       "      <td>-19.766190</td>\n",
       "      <td>-18.301623</td>\n",
       "      <td>-0.277086</td>\n",
       "      <td>-0.257033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.313858</td>\n",
       "      <td>0.036896</td>\n",
       "      <td>-19.509003</td>\n",
       "      <td>-18.329880</td>\n",
       "      <td>-0.279009</td>\n",
       "      <td>-0.257903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.278360</td>\n",
       "      <td>0.035649</td>\n",
       "      <td>-20.753928</td>\n",
       "      <td>-18.169899</td>\n",
       "      <td>-0.286035</td>\n",
       "      <td>-0.255422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.116011</td>\n",
       "      <td>0.035437</td>\n",
       "      <td>-18.603644</td>\n",
       "      <td>-18.422114</td>\n",
       "      <td>-0.257281</td>\n",
       "      <td>-0.259371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.117245</td>\n",
       "      <td>0.036001</td>\n",
       "      <td>-20.061586</td>\n",
       "      <td>-18.221126</td>\n",
       "      <td>-0.292100</td>\n",
       "      <td>-0.256277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.227798</td>\n",
       "      <td>0.036850</td>\n",
       "      <td>-20.452727</td>\n",
       "      <td>-18.212630</td>\n",
       "      <td>-0.272959</td>\n",
       "      <td>-0.257272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.580834</td>\n",
       "      <td>0.042403</td>\n",
       "      <td>-20.877313</td>\n",
       "      <td>-18.178053</td>\n",
       "      <td>-0.290607</td>\n",
       "      <td>-0.255458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.307217</td>\n",
       "      <td>0.045741</td>\n",
       "      <td>-20.888908</td>\n",
       "      <td>-18.183205</td>\n",
       "      <td>-0.288564</td>\n",
       "      <td>-0.255917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.293974</td>\n",
       "      <td>0.035995</td>\n",
       "      <td>-19.776070</td>\n",
       "      <td>-18.282619</td>\n",
       "      <td>-0.278818</td>\n",
       "      <td>-0.257295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.276098</td>\n",
       "      <td>0.047512</td>\n",
       "      <td>-20.440810</td>\n",
       "      <td>-18.154629</td>\n",
       "      <td>-0.280990</td>\n",
       "      <td>-0.255598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_neg_mean_absolute_error  \\\n",
       "0  2.186526    0.036434                    -19.766190   \n",
       "1  2.313858    0.036896                    -19.509003   \n",
       "2  2.278360    0.035649                    -20.753928   \n",
       "3  2.116011    0.035437                    -18.603644   \n",
       "4  2.117245    0.036001                    -20.061586   \n",
       "5  2.227798    0.036850                    -20.452727   \n",
       "6  2.580834    0.042403                    -20.877313   \n",
       "7  2.307217    0.045741                    -20.888908   \n",
       "8  2.293974    0.035995                    -19.776070   \n",
       "9  2.276098    0.047512                    -20.440810   \n",
       "\n",
       "   train_neg_mean_absolute_error  test_neg_mean_absolute_percentage_error  \\\n",
       "0                     -18.301623                                -0.277086   \n",
       "1                     -18.329880                                -0.279009   \n",
       "2                     -18.169899                                -0.286035   \n",
       "3                     -18.422114                                -0.257281   \n",
       "4                     -18.221126                                -0.292100   \n",
       "5                     -18.212630                                -0.272959   \n",
       "6                     -18.178053                                -0.290607   \n",
       "7                     -18.183205                                -0.288564   \n",
       "8                     -18.282619                                -0.278818   \n",
       "9                     -18.154629                                -0.280990   \n",
       "\n",
       "   train_neg_mean_absolute_percentage_error  \n",
       "0                                 -0.257033  \n",
       "1                                 -0.257903  \n",
       "2                                 -0.255422  \n",
       "3                                 -0.259371  \n",
       "4                                 -0.256277  \n",
       "5                                 -0.257272  \n",
       "6                                 -0.255458  \n",
       "7                                 -0.255917  \n",
       "8                                 -0.257295  \n",
       "9                                 -0.255598  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rf_cv_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MAPE - Cross Validation Test: -0.28034500675859364\n",
      "Standard Deviation of MAPE - Cross Validation Test: 0.01024728192931544\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average MAPE - Cross Validation Test: {df_rf_cv_metrics['test_neg_mean_absolute_percentage_error'].mean()}\")\n",
    "print(f\"Standard Deviation of MAPE - Cross Validation Test: {df_rf_cv_metrics['test_neg_mean_absolute_percentage_error'].std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('impute_mode_one_hot',\n",
       "                                                  Pipeline(steps=[('impute_mode',\n",
       "                                                                   Pipeline(steps=[('impute_mode',\n",
       "                                                                                    SimpleImputer(strategy='most_frequent'))])),\n",
       "                                                                  ('one_hot',\n",
       "                                                                   Pipeline(steps=[('cat_encoder',\n",
       "                                                                                    OneHotEncoder(handle_unknown='ignore'))]))]),\n",
       "                                                  ['on_demand', 'hour_group']),\n",
       "                                                 ('impute_median',\n",
       "                                                  Pipeline(steps=[('impute_median',\n",
       "                                                                   SimpleImputer(strategy='median'))]),\n",
       "                                                  ['n_distinct_items',\n",
       "                                                   'distance_km',\n",
       "                                                   'found_rate']),\n",
       "                                                 ('impute_zero',\n",
       "                                                  Pipeline(steps=[('impute_zero',\n",
       "                                                                   SimpleImputer(fill_value=0,\n",
       "                                                                                 strategy='constant'))]),\n",
       "                                                  ['sum_kgs',\n",
       "                                                   'sum_unities'])])),\n",
       "                ('model',\n",
       "                 RandomForestRegressor(max_depth=7, max_leaf_nodes=718,\n",
       "                                       n_estimators=309, random_state=123))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About 57.08333333333333% of our predictions are higher than the real value\n"
     ]
    }
   ],
   "source": [
    "# Generating predictions for X_train\n",
    "y_train_rf_predict = model_rf.predict(X_train)\n",
    "# Calculating proportion of overestimation\n",
    "print(f'About {((y_train_rf_predict >= y_train).sum())/len(y_train)*100}% of our predictions are higher than the real value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that, despite MAPE is a little worse compared to the baseline, the proportion of overestimated predictions is higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) NGBoost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, We'll train a NGBoost model, which is capable of outputing a distribution for each prediction, so that We'll be able to calculate prediction interval, what can be really useful in production enviroment, because Cornershop can now give an interval, instead of a point estimation for the time the order will take to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=4.8731 val_loss=0.0000 scale=1.0000 norm=0.6300\n",
      "[iter 100] loss=4.6717 val_loss=0.0000 scale=1.0000 norm=0.5447\n",
      "[iter 200] loss=4.5897 val_loss=0.0000 scale=1.0000 norm=0.5485\n",
      "[iter 300] loss=4.5594 val_loss=0.0000 scale=1.0000 norm=0.5618\n",
      "[iter 400] loss=4.5485 val_loss=0.0000 scale=1.0000 norm=0.5678\n",
      "[iter 0] loss=4.8759 val_loss=0.0000 scale=1.0000 norm=0.6297\n",
      "[iter 100] loss=4.6766 val_loss=0.0000 scale=1.0000 norm=0.5445\n",
      "[iter 200] loss=4.5975 val_loss=0.0000 scale=1.0000 norm=0.5475\n",
      "[iter 300] loss=4.5662 val_loss=0.0000 scale=1.0000 norm=0.5597\n",
      "[iter 400] loss=4.5549 val_loss=0.0000 scale=2.0000 norm=1.1334\n",
      "[iter 0] loss=4.8676 val_loss=0.0000 scale=1.0000 norm=0.6267\n",
      "[iter 100] loss=4.6655 val_loss=0.0000 scale=1.0000 norm=0.5406\n",
      "[iter 200] loss=4.5803 val_loss=0.0000 scale=1.0000 norm=0.5465\n",
      "[iter 300] loss=4.5531 val_loss=0.0000 scale=1.0000 norm=0.5593\n",
      "[iter 400] loss=4.5430 val_loss=0.0000 scale=2.0000 norm=1.1300\n",
      "[iter 0] loss=4.8758 val_loss=0.0000 scale=1.0000 norm=0.6284\n",
      "[iter 100] loss=4.6740 val_loss=0.0000 scale=1.0000 norm=0.5415\n",
      "[iter 200] loss=4.5920 val_loss=0.0000 scale=1.0000 norm=0.5462\n",
      "[iter 300] loss=4.5625 val_loss=0.0000 scale=1.0000 norm=0.5597\n",
      "[iter 400] loss=4.5513 val_loss=0.0000 scale=1.0000 norm=0.5658\n",
      "[iter 0] loss=4.8687 val_loss=0.0000 scale=1.0000 norm=0.6278\n",
      "[iter 100] loss=4.6704 val_loss=0.0000 scale=1.0000 norm=0.5433\n",
      "[iter 200] loss=4.5780 val_loss=0.0000 scale=2.0000 norm=1.1001\n",
      "[iter 300] loss=4.5546 val_loss=0.0000 scale=1.0000 norm=0.5625\n",
      "[iter 400] loss=4.5457 val_loss=0.0000 scale=2.0000 norm=1.1330\n",
      "[iter 0] loss=4.8659 val_loss=0.0000 scale=1.0000 norm=0.6291\n",
      "[iter 100] loss=4.6670 val_loss=0.0000 scale=1.0000 norm=0.5453\n",
      "[iter 200] loss=4.5766 val_loss=0.0000 scale=1.0000 norm=0.5520\n",
      "[iter 300] loss=4.5524 val_loss=0.0000 scale=2.0000 norm=1.1295\n",
      "[iter 400] loss=4.5433 val_loss=0.0000 scale=1.0000 norm=0.5694\n",
      "[iter 0] loss=4.8693 val_loss=0.0000 scale=1.0000 norm=0.6284\n",
      "[iter 100] loss=4.6671 val_loss=0.0000 scale=1.0000 norm=0.5420\n",
      "[iter 200] loss=4.5791 val_loss=0.0000 scale=1.0000 norm=0.5479\n",
      "[iter 300] loss=4.5530 val_loss=0.0000 scale=2.0000 norm=1.1217\n",
      "[iter 400] loss=4.5430 val_loss=0.0000 scale=1.0000 norm=0.5662\n",
      "[iter 0] loss=4.8630 val_loss=0.0000 scale=1.0000 norm=0.6269\n",
      "[iter 100] loss=4.6619 val_loss=0.0000 scale=1.0000 norm=0.5415\n",
      "[iter 200] loss=4.5800 val_loss=0.0000 scale=1.0000 norm=0.5473\n",
      "[iter 300] loss=4.5527 val_loss=0.0000 scale=1.0000 norm=0.5603\n",
      "[iter 400] loss=4.5427 val_loss=0.0000 scale=1.0000 norm=0.5664\n",
      "[iter 0] loss=4.8729 val_loss=0.0000 scale=1.0000 norm=0.6300\n",
      "[iter 100] loss=4.6731 val_loss=0.0000 scale=1.0000 norm=0.5448\n",
      "[iter 200] loss=4.5912 val_loss=0.0000 scale=1.0000 norm=0.5491\n",
      "[iter 300] loss=4.5603 val_loss=0.0000 scale=1.0000 norm=0.5629\n",
      "[iter 400] loss=4.5500 val_loss=0.0000 scale=1.0000 norm=0.5687\n",
      "[iter 0] loss=4.8700 val_loss=0.0000 scale=1.0000 norm=0.6287\n",
      "[iter 100] loss=4.6660 val_loss=0.0000 scale=1.0000 norm=0.5418\n",
      "[iter 200] loss=4.5834 val_loss=0.0000 scale=1.0000 norm=0.5462\n",
      "[iter 300] loss=4.5540 val_loss=0.0000 scale=2.0000 norm=1.1190\n",
      "[iter 400] loss=4.5436 val_loss=0.0000 scale=1.0000 norm=0.5659\n"
     ]
    }
   ],
   "source": [
    "model_nb = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('model',NGBRegressor(Base=DecisionTreeRegressor(max_depth=5, min_samples_leaf=600,random_state=123),\n",
    "                                                 random_state=123,\n",
    "                                                Dist=LogNormal))])\n",
    "df_rf_nb_metrics = pd.DataFrame(cross_validate(model_nb,\n",
    "                                        X_train,\n",
    "                                        y_train, \n",
    "                                        scoring=['neg_mean_absolute_error', 'neg_mean_absolute_percentage_error'], \n",
    "                                        return_train_score=True, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_neg_mean_absolute_error</th>\n",
       "      <th>train_neg_mean_absolute_error</th>\n",
       "      <th>test_neg_mean_absolute_percentage_error</th>\n",
       "      <th>train_neg_mean_absolute_percentage_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.747421</td>\n",
       "      <td>0.103352</td>\n",
       "      <td>-19.877955</td>\n",
       "      <td>-19.931487</td>\n",
       "      <td>-0.276375</td>\n",
       "      <td>-0.274594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.048690</td>\n",
       "      <td>0.141655</td>\n",
       "      <td>-19.136172</td>\n",
       "      <td>-20.036376</td>\n",
       "      <td>-0.271892</td>\n",
       "      <td>-0.276238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.182058</td>\n",
       "      <td>0.150833</td>\n",
       "      <td>-21.170471</td>\n",
       "      <td>-19.800007</td>\n",
       "      <td>-0.286607</td>\n",
       "      <td>-0.273285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.890416</td>\n",
       "      <td>0.141449</td>\n",
       "      <td>-18.852807</td>\n",
       "      <td>-20.015819</td>\n",
       "      <td>-0.257770</td>\n",
       "      <td>-0.276485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.215286</td>\n",
       "      <td>0.122172</td>\n",
       "      <td>-20.453957</td>\n",
       "      <td>-19.853026</td>\n",
       "      <td>-0.293276</td>\n",
       "      <td>-0.273466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.529412</td>\n",
       "      <td>0.109963</td>\n",
       "      <td>-20.642691</td>\n",
       "      <td>-19.814938</td>\n",
       "      <td>-0.272081</td>\n",
       "      <td>-0.274633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12.486743</td>\n",
       "      <td>0.106846</td>\n",
       "      <td>-20.981803</td>\n",
       "      <td>-19.835037</td>\n",
       "      <td>-0.289344</td>\n",
       "      <td>-0.273698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12.478712</td>\n",
       "      <td>0.105654</td>\n",
       "      <td>-21.062606</td>\n",
       "      <td>-19.815347</td>\n",
       "      <td>-0.291492</td>\n",
       "      <td>-0.273370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13.571088</td>\n",
       "      <td>0.137985</td>\n",
       "      <td>-19.987676</td>\n",
       "      <td>-19.921240</td>\n",
       "      <td>-0.280282</td>\n",
       "      <td>-0.275080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13.216227</td>\n",
       "      <td>0.102461</td>\n",
       "      <td>-20.875762</td>\n",
       "      <td>-19.820506</td>\n",
       "      <td>-0.284186</td>\n",
       "      <td>-0.273671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fit_time  score_time  test_neg_mean_absolute_error  \\\n",
       "0  12.747421    0.103352                    -19.877955   \n",
       "1  12.048690    0.141655                    -19.136172   \n",
       "2  14.182058    0.150833                    -21.170471   \n",
       "3  13.890416    0.141449                    -18.852807   \n",
       "4  13.215286    0.122172                    -20.453957   \n",
       "5  12.529412    0.109963                    -20.642691   \n",
       "6  12.486743    0.106846                    -20.981803   \n",
       "7  12.478712    0.105654                    -21.062606   \n",
       "8  13.571088    0.137985                    -19.987676   \n",
       "9  13.216227    0.102461                    -20.875762   \n",
       "\n",
       "   train_neg_mean_absolute_error  test_neg_mean_absolute_percentage_error  \\\n",
       "0                     -19.931487                                -0.276375   \n",
       "1                     -20.036376                                -0.271892   \n",
       "2                     -19.800007                                -0.286607   \n",
       "3                     -20.015819                                -0.257770   \n",
       "4                     -19.853026                                -0.293276   \n",
       "5                     -19.814938                                -0.272081   \n",
       "6                     -19.835037                                -0.289344   \n",
       "7                     -19.815347                                -0.291492   \n",
       "8                     -19.921240                                -0.280282   \n",
       "9                     -19.820506                                -0.284186   \n",
       "\n",
       "   train_neg_mean_absolute_percentage_error  \n",
       "0                                 -0.274594  \n",
       "1                                 -0.276238  \n",
       "2                                 -0.273285  \n",
       "3                                 -0.276485  \n",
       "4                                 -0.273466  \n",
       "5                                 -0.274633  \n",
       "6                                 -0.273698  \n",
       "7                                 -0.273370  \n",
       "8                                 -0.275080  \n",
       "9                                 -0.273671  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rf_nb_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MAPE - Cross Validation Test: -0.2803305443860043\n",
      "Standard Deviation of MAPE - Cross Validation Test: 0.011013031804846442\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average MAPE - Cross Validation Test: {df_rf_nb_metrics['test_neg_mean_absolute_percentage_error'].mean()}\")\n",
    "print(f\"Standard Deviation of MAPE - Cross Validation Test: {df_rf_nb_metrics['test_neg_mean_absolute_percentage_error'].std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=4.8702 val_loss=0.0000 scale=1.0000 norm=0.6286\n",
      "[iter 100] loss=4.6638 val_loss=0.0000 scale=1.0000 norm=0.5410\n",
      "[iter 200] loss=4.5812 val_loss=0.0000 scale=2.0000 norm=1.0922\n",
      "[iter 300] loss=4.5515 val_loss=0.0000 scale=1.0000 norm=0.5613\n",
      "[iter 400] loss=4.5426 val_loss=0.0000 scale=2.0000 norm=1.1339\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('impute_mode_one_hot',\n",
       "                                                  Pipeline(steps=[('impute_mode',\n",
       "                                                                   Pipeline(steps=[('impute_mode',\n",
       "                                                                                    SimpleImputer(strategy='most_frequent'))])),\n",
       "                                                                  ('one_hot',\n",
       "                                                                   Pipeline(steps=[('cat_encoder',\n",
       "                                                                                    OneHotEncoder(handle_unknown='ignore'))]))]),\n",
       "                                                  ['on_demand', 'hour_group']),\n",
       "                                                 ('impute_median',\n",
       "                                                  Pipeline(steps=[('impute_...\n",
       "                                                 ('impute_zero',\n",
       "                                                  Pipeline(steps=[('impute_zero',\n",
       "                                                                   SimpleImputer(fill_value=0,\n",
       "                                                                                 strategy='constant'))]),\n",
       "                                                  ['sum_kgs',\n",
       "                                                   'sum_unities'])])),\n",
       "                ('model',\n",
       "                 NGBRegressor(Base=DecisionTreeRegressor(max_depth=5,\n",
       "                                                         min_samples_leaf=600,\n",
       "                                                         random_state=123),\n",
       "                              Dist=<class 'ngboost.distns.distn.Distn.uncensor.<locals>.DistWithUncensoredScore'>,\n",
       "                              random_state=RandomState(MT19937) at 0x1603E6440))])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nb.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About 57.41666666666667% of our predictions are higher than the real value\n"
     ]
    }
   ],
   "source": [
    "y_train_nb_predict = model_nb.predict(X_train)\n",
    "print(f'About {((y_train_nb_predict >= y_train).sum())/len(y_train)*100}% of our predictions are higher than the real value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate prediction interval, We'll need to apply `.pred_dist()` method and then extract the distribution parameters for each prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing step\n",
    "X_train_preprocessed = model_nb['preprocessor'].transform(X_train)\n",
    "# get parameters \n",
    "y_train_prob = model_nb['model'].pred_dist(X_train_preprocessed).params\n",
    "predictions = pd.DataFrame(y_train_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s</th>\n",
       "      <th>scale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.390934</td>\n",
       "      <td>44.356750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.260647</td>\n",
       "      <td>123.058273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.408346</td>\n",
       "      <td>56.740349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.234675</td>\n",
       "      <td>107.306261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.330448</td>\n",
       "      <td>84.836854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>0.338832</td>\n",
       "      <td>55.881411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>0.327237</td>\n",
       "      <td>75.126496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>0.394121</td>\n",
       "      <td>48.546872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>0.292796</td>\n",
       "      <td>71.703699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>0.261597</td>\n",
       "      <td>82.587356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             s       scale\n",
       "0     0.390934   44.356750\n",
       "1     0.260647  123.058273\n",
       "2     0.408346   56.740349\n",
       "3     0.234675  107.306261\n",
       "4     0.330448   84.836854\n",
       "...        ...         ...\n",
       "5995  0.338832   55.881411\n",
       "5996  0.327237   75.126496\n",
       "5997  0.394121   48.546872\n",
       "5998  0.292796   71.703699\n",
       "5999  0.261597   82.587356\n",
       "\n",
       "[6000 rows x 2 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each row We have `s` and `scale` values. As We defined the target to have a normal distribution, We can apply these parameters and get the interval using `scipy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#create 95% confidence interval \n",
    "predictions['interval'] = predictions.apply(lambda x: st.lognorm.interval(alpha=0.95, s=x['s'], scale=x['scale']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s</th>\n",
       "      <th>scale</th>\n",
       "      <th>interval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.390934</td>\n",
       "      <td>44.356750</td>\n",
       "      <td>(20.615620568217647, 95.43837093908799)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.260647</td>\n",
       "      <td>123.058273</td>\n",
       "      <td>(73.83247350836697, 205.10404033403873)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.408346</td>\n",
       "      <td>56.740349</td>\n",
       "      <td>(25.486302670224685, 126.32146842445916)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.234675</td>\n",
       "      <td>107.306261</td>\n",
       "      <td>(67.74382628170208, 169.97318179350103)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.330448</td>\n",
       "      <td>84.836854</td>\n",
       "      <td>(44.392176379062086, 162.12973513010948)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>0.338832</td>\n",
       "      <td>55.881411</td>\n",
       "      <td>(28.76425433781782, 108.56294117130143)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>0.327237</td>\n",
       "      <td>75.126496</td>\n",
       "      <td>(39.55930472505586, 142.671626377914)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>0.394121</td>\n",
       "      <td>48.546872</td>\n",
       "      <td>(22.42255262058418, 105.10840472823831)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>0.292796</td>\n",
       "      <td>71.703699</td>\n",
       "      <td>(40.39363320034354, 127.28294235657191)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>0.261597</td>\n",
       "      <td>82.587356</td>\n",
       "      <td>(49.45854945646476, 137.90682032701915)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             s       scale                                  interval\n",
       "0     0.390934   44.356750   (20.615620568217647, 95.43837093908799)\n",
       "1     0.260647  123.058273   (73.83247350836697, 205.10404033403873)\n",
       "2     0.408346   56.740349  (25.486302670224685, 126.32146842445916)\n",
       "3     0.234675  107.306261   (67.74382628170208, 169.97318179350103)\n",
       "4     0.330448   84.836854  (44.392176379062086, 162.12973513010948)\n",
       "...        ...         ...                                       ...\n",
       "5995  0.338832   55.881411   (28.76425433781782, 108.56294117130143)\n",
       "5996  0.327237   75.126496     (39.55930472505586, 142.671626377914)\n",
       "5997  0.394121   48.546872   (22.42255262058418, 105.10840472823831)\n",
       "5998  0.292796   71.703699   (40.39363320034354, 127.28294235657191)\n",
       "5999  0.261597   82.587356   (49.45854945646476, 137.90682032701915)\n",
       "\n",
       "[6000 rows x 3 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the interval is pretty large in some cases, which says that We need more data and more features to accomplish the task completely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NGBoost performed worse than `Random Forest Regressor` and `Linear Regression` comparing `MAPE`. When It comes `Overestimation Rate`, NGBoost is the best model. Because the differente in MAPE is not so large between models and Overestimation Rate can impact a lot in User Journey/Experience, We will choose NGBoost. Besides, NGBoost is able to give distributions instead of point estimates, so We'll be able to estimate confidence intervals, that may be more suitable in delivery applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=4.8250 val_loss=0.0000 scale=512.0000 norm=316.1187\n",
      "== Quitting at iteration / GRAD 0\n",
      "[iter 0] loss=4.8493 val_loss=0.0000 scale=1.0000 norm=0.6324\n",
      "[iter 100] loss=4.7143 val_loss=0.0000 scale=2.0000 norm=1.1536\n",
      "[iter 200] loss=4.6614 val_loss=0.0000 scale=1.0000 norm=0.5800\n",
      "[iter 300] loss=4.6381 val_loss=0.0000 scale=2.0000 norm=1.1670\n",
      "[iter 400] loss=4.6246 val_loss=0.0000 scale=2.0000 norm=1.1710\n",
      "[iter 0] loss=4.8654 val_loss=0.0000 scale=1.0000 norm=0.6309\n",
      "[iter 100] loss=4.6833 val_loss=0.0000 scale=1.0000 norm=0.5537\n",
      "[iter 200] loss=4.6179 val_loss=0.0000 scale=1.0000 norm=0.5533\n",
      "[iter 300] loss=4.5859 val_loss=0.0000 scale=1.0000 norm=0.5620\n",
      "[iter 400] loss=4.5711 val_loss=0.0000 scale=1.0000 norm=0.5693\n",
      "[iter 0] loss=4.8773 val_loss=0.0000 scale=1.0000 norm=0.6323\n",
      "[iter 100] loss=4.6774 val_loss=0.0000 scale=1.0000 norm=0.5478\n",
      "[iter 200] loss=4.5970 val_loss=0.0000 scale=2.0000 norm=1.1042\n",
      "[iter 300] loss=4.5677 val_loss=0.0000 scale=1.0000 norm=0.5653\n",
      "[iter 400] loss=4.5568 val_loss=0.0000 scale=1.0000 norm=0.5719\n",
      "[iter 0] loss=4.8731 val_loss=0.0000 scale=1.0000 norm=0.6300\n",
      "[iter 100] loss=4.6717 val_loss=0.0000 scale=1.0000 norm=0.5447\n",
      "[iter 200] loss=4.5897 val_loss=0.0000 scale=1.0000 norm=0.5485\n",
      "[iter 300] loss=4.5594 val_loss=0.0000 scale=1.0000 norm=0.5618\n",
      "[iter 400] loss=4.5485 val_loss=0.0000 scale=1.0000 norm=0.5678\n",
      "[iter 0] loss=4.8487 val_loss=0.0000 scale=256.0000 norm=158.0449\n",
      "== Quitting at iteration / GRAD 0\n",
      "[iter 0] loss=4.8581 val_loss=0.0000 scale=1.0000 norm=0.6313\n",
      "[iter 100] loss=4.7178 val_loss=0.0000 scale=1.0000 norm=0.5740\n",
      "[iter 200] loss=4.6691 val_loss=0.0000 scale=2.0000 norm=1.1546\n",
      "[iter 300] loss=4.6464 val_loss=0.0000 scale=1.0000 norm=0.5801\n",
      "[iter 400] loss=4.6333 val_loss=0.0000 scale=1.0000 norm=0.5818\n",
      "[iter 0] loss=4.8705 val_loss=0.0000 scale=1.0000 norm=0.6303\n",
      "[iter 100] loss=4.6919 val_loss=0.0000 scale=1.0000 norm=0.5519\n",
      "[iter 200] loss=4.6221 val_loss=0.0000 scale=1.0000 norm=0.5498\n",
      "[iter 300] loss=4.5895 val_loss=0.0000 scale=1.0000 norm=0.5585\n",
      "[iter 400] loss=4.5751 val_loss=0.0000 scale=2.0000 norm=1.1300\n",
      "[iter 0] loss=4.8809 val_loss=0.0000 scale=1.0000 norm=0.6319\n",
      "[iter 100] loss=4.6822 val_loss=0.0000 scale=1.0000 norm=0.5467\n",
      "[iter 200] loss=4.6016 val_loss=0.0000 scale=2.0000 norm=1.1023\n",
      "[iter 300] loss=4.5730 val_loss=0.0000 scale=1.0000 norm=0.5637\n",
      "[iter 400] loss=4.5623 val_loss=0.0000 scale=1.0000 norm=0.5702\n",
      "[iter 0] loss=4.8759 val_loss=0.0000 scale=1.0000 norm=0.6297\n",
      "[iter 100] loss=4.6766 val_loss=0.0000 scale=1.0000 norm=0.5445\n",
      "[iter 200] loss=4.5975 val_loss=0.0000 scale=1.0000 norm=0.5475\n",
      "[iter 300] loss=4.5662 val_loss=0.0000 scale=1.0000 norm=0.5597\n",
      "[iter 400] loss=4.5549 val_loss=0.0000 scale=2.0000 norm=1.1334\n",
      "[iter 0] loss=4.8487 val_loss=0.0000 scale=256.0000 norm=158.0449\n",
      "== Quitting at iteration / GRAD 0\n",
      "[iter 0] loss=4.8313 val_loss=0.0000 scale=1.0000 norm=0.6213\n",
      "[iter 100] loss=4.6947 val_loss=0.0000 scale=1.0000 norm=0.5642\n",
      "[iter 200] loss=4.6390 val_loss=0.0000 scale=2.0000 norm=1.1342\n",
      "[iter 300] loss=4.6116 val_loss=0.0000 scale=2.0000 norm=1.1451\n",
      "[iter 400] loss=4.5965 val_loss=0.0000 scale=1.0000 norm=0.5758\n",
      "[iter 0] loss=4.8551 val_loss=0.0000 scale=1.0000 norm=0.6246\n",
      "[iter 100] loss=4.6730 val_loss=0.0000 scale=1.0000 norm=0.5452\n",
      "[iter 200] loss=4.6034 val_loss=0.0000 scale=1.0000 norm=0.5446\n",
      "[iter 300] loss=4.5730 val_loss=0.0000 scale=1.0000 norm=0.5538\n",
      "[iter 400] loss=4.5574 val_loss=0.0000 scale=1.0000 norm=0.5608\n",
      "[iter 0] loss=4.8703 val_loss=0.0000 scale=1.0000 norm=0.6280\n",
      "[iter 100] loss=4.6688 val_loss=0.0000 scale=1.0000 norm=0.5425\n",
      "[iter 200] loss=4.5817 val_loss=0.0000 scale=2.0000 norm=1.0988\n",
      "[iter 300] loss=4.5565 val_loss=0.0000 scale=1.0000 norm=0.5619\n",
      "[iter 400] loss=4.5466 val_loss=0.0000 scale=1.0000 norm=0.5679\n",
      "[iter 0] loss=4.8676 val_loss=0.0000 scale=1.0000 norm=0.6267\n",
      "[iter 100] loss=4.6655 val_loss=0.0000 scale=1.0000 norm=0.5406\n",
      "[iter 200] loss=4.5803 val_loss=0.0000 scale=1.0000 norm=0.5465\n",
      "[iter 300] loss=4.5531 val_loss=0.0000 scale=1.0000 norm=0.5593\n",
      "[iter 400] loss=4.5430 val_loss=0.0000 scale=2.0000 norm=1.1300\n",
      "[iter 0] loss=4.8487 val_loss=0.0000 scale=256.0000 norm=158.0449\n",
      "== Quitting at iteration / GRAD 0\n",
      "[iter 0] loss=4.8520 val_loss=0.0000 scale=1.0000 norm=0.6260\n",
      "[iter 100] loss=4.7158 val_loss=0.0000 scale=1.0000 norm=0.5663\n",
      "[iter 200] loss=4.6586 val_loss=0.0000 scale=2.0000 norm=1.1332\n",
      "[iter 300] loss=4.6336 val_loss=0.0000 scale=2.0000 norm=1.1435\n",
      "[iter 400] loss=4.6197 val_loss=0.0000 scale=1.0000 norm=0.5746\n",
      "[iter 0] loss=4.8702 val_loss=0.0000 scale=1.0000 norm=0.6279\n",
      "[iter 100] loss=4.6878 val_loss=0.0000 scale=1.0000 norm=0.5474\n",
      "[iter 200] loss=4.6192 val_loss=0.0000 scale=1.0000 norm=0.5470\n",
      "[iter 300] loss=4.5892 val_loss=0.0000 scale=1.0000 norm=0.5562\n",
      "[iter 400] loss=4.5741 val_loss=0.0000 scale=1.0000 norm=0.5628\n",
      "[iter 0] loss=4.8807 val_loss=0.0000 scale=1.0000 norm=0.6302\n",
      "[iter 100] loss=4.6793 val_loss=0.0000 scale=1.0000 norm=0.5435\n",
      "[iter 200] loss=4.5995 val_loss=0.0000 scale=1.0000 norm=0.5487\n",
      "[iter 300] loss=4.5715 val_loss=0.0000 scale=2.0000 norm=1.1215\n",
      "[iter 400] loss=4.5604 val_loss=0.0000 scale=1.0000 norm=0.5678\n",
      "[iter 0] loss=4.8758 val_loss=0.0000 scale=1.0000 norm=0.6284\n",
      "[iter 100] loss=4.6740 val_loss=0.0000 scale=1.0000 norm=0.5415\n",
      "[iter 200] loss=4.5920 val_loss=0.0000 scale=1.0000 norm=0.5462\n",
      "[iter 300] loss=4.5625 val_loss=0.0000 scale=1.0000 norm=0.5597\n",
      "[iter 400] loss=4.5513 val_loss=0.0000 scale=1.0000 norm=0.5658\n",
      "[iter 0] loss=4.8487 val_loss=0.0000 scale=256.0000 norm=158.0449\n",
      "== Quitting at iteration / GRAD 0\n",
      "[iter 0] loss=4.8520 val_loss=0.0000 scale=1.0000 norm=0.6260\n",
      "[iter 100] loss=4.7158 val_loss=0.0000 scale=1.0000 norm=0.5663\n",
      "[iter 200] loss=4.6586 val_loss=0.0000 scale=2.0000 norm=1.1332\n",
      "[iter 300] loss=4.6336 val_loss=0.0000 scale=2.0000 norm=1.1435\n",
      "[iter 400] loss=4.6197 val_loss=0.0000 scale=1.0000 norm=0.5746\n",
      "[iter 0] loss=4.8571 val_loss=0.0000 scale=1.0000 norm=0.6268\n",
      "[iter 100] loss=4.6828 val_loss=0.0000 scale=1.0000 norm=0.5507\n",
      "[iter 200] loss=4.6116 val_loss=0.0000 scale=1.0000 norm=0.5496\n",
      "[iter 300] loss=4.5788 val_loss=0.0000 scale=2.0000 norm=1.1158\n",
      "[iter 400] loss=4.5643 val_loss=0.0000 scale=2.0000 norm=1.1288\n",
      "[iter 0] loss=4.8717 val_loss=0.0000 scale=1.0000 norm=0.6295\n",
      "[iter 100] loss=4.6748 val_loss=0.0000 scale=1.0000 norm=0.5453\n",
      "[iter 200] loss=4.5922 val_loss=0.0000 scale=1.0000 norm=0.5506\n",
      "[iter 300] loss=4.5634 val_loss=0.0000 scale=2.0000 norm=1.1268\n",
      "[iter 400] loss=4.5521 val_loss=0.0000 scale=1.0000 norm=0.5696\n",
      "[iter 0] loss=4.8687 val_loss=0.0000 scale=1.0000 norm=0.6278\n",
      "[iter 100] loss=4.6704 val_loss=0.0000 scale=1.0000 norm=0.5433\n",
      "[iter 200] loss=4.5780 val_loss=0.0000 scale=2.0000 norm=1.1001\n",
      "[iter 300] loss=4.5546 val_loss=0.0000 scale=1.0000 norm=0.5625\n",
      "[iter 400] loss=4.5457 val_loss=0.0000 scale=2.0000 norm=1.1330\n",
      "[iter 0] loss=4.8487 val_loss=0.0000 scale=256.0000 norm=158.0449\n",
      "== Quitting at iteration / GRAD 0\n",
      "[iter 0] loss=4.8520 val_loss=0.0000 scale=1.0000 norm=0.6260\n",
      "[iter 100] loss=4.7158 val_loss=0.0000 scale=1.0000 norm=0.5663\n",
      "[iter 200] loss=4.6586 val_loss=0.0000 scale=2.0000 norm=1.1332\n",
      "[iter 300] loss=4.6336 val_loss=0.0000 scale=2.0000 norm=1.1435\n",
      "[iter 400] loss=4.6197 val_loss=0.0000 scale=1.0000 norm=0.5746\n",
      "[iter 0] loss=4.8519 val_loss=0.0000 scale=1.0000 norm=0.6292\n",
      "[iter 100] loss=4.6737 val_loss=0.0000 scale=2.0000 norm=1.1071\n",
      "[iter 200] loss=4.6007 val_loss=0.0000 scale=1.0000 norm=0.5536\n",
      "[iter 300] loss=4.5703 val_loss=0.0000 scale=1.0000 norm=0.5635\n",
      "[iter 400] loss=4.5568 val_loss=0.0000 scale=1.0000 norm=0.5693\n",
      "[iter 0] loss=4.8680 val_loss=0.0000 scale=1.0000 norm=0.6312\n",
      "[iter 100] loss=4.6709 val_loss=0.0000 scale=1.0000 norm=0.5484\n",
      "[iter 200] loss=4.5896 val_loss=0.0000 scale=1.0000 norm=0.5536\n",
      "[iter 300] loss=4.5616 val_loss=0.0000 scale=1.0000 norm=0.5657\n",
      "[iter 400] loss=4.5516 val_loss=0.0000 scale=1.0000 norm=0.5724\n",
      "[iter 0] loss=4.8659 val_loss=0.0000 scale=1.0000 norm=0.6291\n",
      "[iter 100] loss=4.6670 val_loss=0.0000 scale=1.0000 norm=0.5453\n",
      "[iter 200] loss=4.5766 val_loss=0.0000 scale=1.0000 norm=0.5520\n",
      "[iter 300] loss=4.5524 val_loss=0.0000 scale=2.0000 norm=1.1295\n",
      "[iter 400] loss=4.5433 val_loss=0.0000 scale=1.0000 norm=0.5694\n",
      "[iter 0] loss=4.8487 val_loss=0.0000 scale=256.0000 norm=158.0449\n",
      "== Quitting at iteration / GRAD 0\n",
      "[iter 0] loss=4.8520 val_loss=0.0000 scale=1.0000 norm=0.6260\n",
      "[iter 100] loss=4.7158 val_loss=0.0000 scale=1.0000 norm=0.5663\n",
      "[iter 200] loss=4.6586 val_loss=0.0000 scale=2.0000 norm=1.1332\n",
      "[iter 300] loss=4.6336 val_loss=0.0000 scale=2.0000 norm=1.1435\n",
      "[iter 400] loss=4.6197 val_loss=0.0000 scale=1.0000 norm=0.5746\n",
      "[iter 0] loss=4.8519 val_loss=0.0000 scale=1.0000 norm=0.6292\n",
      "[iter 100] loss=4.6737 val_loss=0.0000 scale=2.0000 norm=1.1071\n",
      "[iter 200] loss=4.6007 val_loss=0.0000 scale=1.0000 norm=0.5536\n",
      "[iter 300] loss=4.5703 val_loss=0.0000 scale=1.0000 norm=0.5635\n",
      "[iter 400] loss=4.5568 val_loss=0.0000 scale=1.0000 norm=0.5693\n",
      "[iter 0] loss=4.8725 val_loss=0.0000 scale=1.0000 norm=0.6303\n",
      "[iter 100] loss=4.6697 val_loss=0.0000 scale=1.0000 norm=0.5433\n",
      "[iter 200] loss=4.5880 val_loss=0.0000 scale=1.0000 norm=0.5482\n",
      "[iter 300] loss=4.5594 val_loss=0.0000 scale=2.0000 norm=1.1244\n",
      "[iter 400] loss=4.5490 val_loss=0.0000 scale=1.0000 norm=0.5691\n",
      "[iter 0] loss=4.8693 val_loss=0.0000 scale=1.0000 norm=0.6284\n",
      "[iter 100] loss=4.6671 val_loss=0.0000 scale=1.0000 norm=0.5420\n",
      "[iter 200] loss=4.5791 val_loss=0.0000 scale=1.0000 norm=0.5479\n",
      "[iter 300] loss=4.5530 val_loss=0.0000 scale=2.0000 norm=1.1217\n",
      "[iter 400] loss=4.5430 val_loss=0.0000 scale=1.0000 norm=0.5662\n",
      "[iter 0] loss=4.8487 val_loss=0.0000 scale=256.0000 norm=158.0449\n",
      "== Quitting at iteration / GRAD 0\n",
      "[iter 0] loss=4.8520 val_loss=0.0000 scale=1.0000 norm=0.6260\n",
      "[iter 100] loss=4.7158 val_loss=0.0000 scale=1.0000 norm=0.5663\n",
      "[iter 200] loss=4.6586 val_loss=0.0000 scale=2.0000 norm=1.1332\n",
      "[iter 300] loss=4.6336 val_loss=0.0000 scale=2.0000 norm=1.1435\n",
      "[iter 400] loss=4.6197 val_loss=0.0000 scale=1.0000 norm=0.5746\n",
      "[iter 0] loss=4.8519 val_loss=0.0000 scale=1.0000 norm=0.6292\n",
      "[iter 100] loss=4.6737 val_loss=0.0000 scale=2.0000 norm=1.1071\n",
      "[iter 200] loss=4.6007 val_loss=0.0000 scale=1.0000 norm=0.5536\n",
      "[iter 300] loss=4.5703 val_loss=0.0000 scale=1.0000 norm=0.5635\n",
      "[iter 400] loss=4.5568 val_loss=0.0000 scale=1.0000 norm=0.5693\n",
      "[iter 0] loss=4.8643 val_loss=0.0000 scale=1.0000 norm=0.6286\n",
      "[iter 100] loss=4.6657 val_loss=0.0000 scale=1.0000 norm=0.5440\n",
      "[iter 200] loss=4.5908 val_loss=0.0000 scale=1.0000 norm=0.5479\n",
      "[iter 300] loss=4.5598 val_loss=0.0000 scale=1.0000 norm=0.5613\n",
      "[iter 400] loss=4.5488 val_loss=0.0000 scale=2.0000 norm=1.1365\n",
      "[iter 0] loss=4.8630 val_loss=0.0000 scale=1.0000 norm=0.6269\n",
      "[iter 100] loss=4.6619 val_loss=0.0000 scale=1.0000 norm=0.5415\n",
      "[iter 200] loss=4.5800 val_loss=0.0000 scale=1.0000 norm=0.5473\n",
      "[iter 300] loss=4.5527 val_loss=0.0000 scale=1.0000 norm=0.5603\n",
      "[iter 400] loss=4.5427 val_loss=0.0000 scale=1.0000 norm=0.5664\n",
      "[iter 0] loss=4.8487 val_loss=0.0000 scale=256.0000 norm=158.0449\n",
      "== Quitting at iteration / GRAD 0\n",
      "[iter 0] loss=4.8520 val_loss=0.0000 scale=1.0000 norm=0.6260\n",
      "[iter 100] loss=4.7158 val_loss=0.0000 scale=1.0000 norm=0.5663\n",
      "[iter 200] loss=4.6586 val_loss=0.0000 scale=2.0000 norm=1.1332\n",
      "[iter 300] loss=4.6336 val_loss=0.0000 scale=2.0000 norm=1.1435\n",
      "[iter 400] loss=4.6197 val_loss=0.0000 scale=1.0000 norm=0.5746\n",
      "[iter 0] loss=4.8519 val_loss=0.0000 scale=1.0000 norm=0.6292\n",
      "[iter 100] loss=4.6737 val_loss=0.0000 scale=2.0000 norm=1.1071\n",
      "[iter 200] loss=4.6007 val_loss=0.0000 scale=1.0000 norm=0.5536\n",
      "[iter 300] loss=4.5703 val_loss=0.0000 scale=1.0000 norm=0.5635\n",
      "[iter 400] loss=4.5568 val_loss=0.0000 scale=1.0000 norm=0.5693\n",
      "[iter 0] loss=4.8643 val_loss=0.0000 scale=1.0000 norm=0.6286\n",
      "[iter 100] loss=4.6657 val_loss=0.0000 scale=1.0000 norm=0.5440\n",
      "[iter 200] loss=4.5908 val_loss=0.0000 scale=1.0000 norm=0.5479\n",
      "[iter 300] loss=4.5598 val_loss=0.0000 scale=1.0000 norm=0.5613\n",
      "[iter 400] loss=4.5488 val_loss=0.0000 scale=2.0000 norm=1.1365\n",
      "[iter 0] loss=4.8729 val_loss=0.0000 scale=1.0000 norm=0.6300\n",
      "[iter 100] loss=4.6731 val_loss=0.0000 scale=1.0000 norm=0.5448\n",
      "[iter 200] loss=4.5912 val_loss=0.0000 scale=1.0000 norm=0.5491\n",
      "[iter 300] loss=4.5603 val_loss=0.0000 scale=1.0000 norm=0.5629\n",
      "[iter 400] loss=4.5500 val_loss=0.0000 scale=1.0000 norm=0.5687\n",
      "[iter 0] loss=4.8487 val_loss=0.0000 scale=256.0000 norm=158.0449\n",
      "== Quitting at iteration / GRAD 0\n",
      "[iter 0] loss=4.8520 val_loss=0.0000 scale=1.0000 norm=0.6260\n",
      "[iter 100] loss=4.7158 val_loss=0.0000 scale=1.0000 norm=0.5663\n",
      "[iter 200] loss=4.6586 val_loss=0.0000 scale=2.0000 norm=1.1332\n",
      "[iter 300] loss=4.6336 val_loss=0.0000 scale=2.0000 norm=1.1435\n",
      "[iter 400] loss=4.6197 val_loss=0.0000 scale=1.0000 norm=0.5746\n",
      "[iter 0] loss=4.8519 val_loss=0.0000 scale=1.0000 norm=0.6292\n",
      "[iter 100] loss=4.6737 val_loss=0.0000 scale=2.0000 norm=1.1071\n",
      "[iter 200] loss=4.6007 val_loss=0.0000 scale=1.0000 norm=0.5536\n",
      "[iter 300] loss=4.5703 val_loss=0.0000 scale=1.0000 norm=0.5635\n",
      "[iter 400] loss=4.5568 val_loss=0.0000 scale=1.0000 norm=0.5693\n",
      "[iter 0] loss=4.8643 val_loss=0.0000 scale=1.0000 norm=0.6286\n",
      "[iter 100] loss=4.6657 val_loss=0.0000 scale=1.0000 norm=0.5440\n",
      "[iter 200] loss=4.5908 val_loss=0.0000 scale=1.0000 norm=0.5479\n",
      "[iter 300] loss=4.5598 val_loss=0.0000 scale=1.0000 norm=0.5613\n",
      "[iter 400] loss=4.5488 val_loss=0.0000 scale=2.0000 norm=1.1365\n",
      "[iter 0] loss=4.8700 val_loss=0.0000 scale=1.0000 norm=0.6287\n",
      "[iter 100] loss=4.6660 val_loss=0.0000 scale=1.0000 norm=0.5418\n",
      "[iter 200] loss=4.5834 val_loss=0.0000 scale=1.0000 norm=0.5462\n",
      "[iter 300] loss=4.5540 val_loss=0.0000 scale=2.0000 norm=1.1190\n",
      "[iter 400] loss=4.5436 val_loss=0.0000 scale=1.0000 norm=0.5659\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_learning_curve(model_nb, X_train, y_train, 10, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Learning Cruve, It seems that validation error and Training error did not reache the minimum error yet, which means more data would be useful in this case. Furthermore, maybe more features help to improve performance as well.\n",
    "If this model was going to be put into production, We would have to wait to get more data in order to start the retraining system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Permutation Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just saving name of features after transformers\n",
    "one_hot_names = model_nb.steps[0][1].transformers_[0][1][1].get_feature_names_out().tolist()\n",
    "median_names = model_nb.steps[0][1].transformers_[1][1][0].feature_names_in_.tolist()\n",
    "zero_names = model_nb.steps[0][1].transformers_[2][1][0].feature_names_in_.tolist()\n",
    "columns_after_transformation = one_hot_names+median_names+zero_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_permutation_importance(model_nb, X=X_train, y=y_train, columns=all_columns, n_repeats=5, scoring='neg_mean_squared_error', n_best=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above We can see the ranking of best features. Top 3 are: `n_distinct_items`, `on_demand` and `found_rate`. But `n_distinct_items` is the most important with clearence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Test and Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1) Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering submission\n",
    "df_submission = pipe_feature_engineering.transform(df_submission)\n",
    "# Selecting only used features\n",
    "X_submission = df_submission[all_columns]\n",
    "# getting point estimates for total_miutes\n",
    "y_predictions_submission = model_nb.predict(X_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data frame with order_id and estimates\n",
    "df_submission_predictions = pd.DataFrame({'order_id':df_submission['order_id'], 'total_minutes':y_predictions_submission})\n",
    "# Getting confidence interval for total_minutes (95%)\n",
    "df_predictions_with_interval = get_intervals(model=model_nb, X=X_submission, confidence=0.95)\n",
    "# lower bound\n",
    "df_submission_predictions['total_minutes_lower_bound'] = df_predictions_with_interval['interval'].apply(lambda x: x[0])\n",
    "# upper bound\n",
    "df_submission_predictions['total_minutes_upper_bound'] = df_predictions_with_interval['interval'].apply(lambda x: x[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps/ Recommendations"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a6f7b07b20f4cf172f035b867796e18e995318450c83b6d17ec9b748e82eefe6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('case_cornershop_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
